{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   date       913000 non-null  datetime64[ns]\n",
      " 1   store      913000 non-null  int64         \n",
      " 2   item       913000 non-null  int64         \n",
      " 3   sales      913000 non-null  int64         \n",
      " 4   year_week  913000 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(1)\n",
      "memory usage: 34.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45000 entries, 0 to 44999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      45000 non-null  int64 \n",
      " 1   date    45000 non-null  object\n",
      " 2   store   45000 non-null  int64 \n",
      " 3   item    45000 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45000 entries, 0 to 44999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   id      45000 non-null  int64\n",
      " 1   sales   45000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 703.2 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        date  store  item  sales year_week\n",
       " 0 2013-01-01      1     1     13    2013-1\n",
       " 1 2013-01-02      1     1     11    2013-1\n",
       " 2 2013-01-03      1     1     14    2013-1\n",
       " 3 2013-01-04      1     1     13    2013-1\n",
       " 4 2013-01-05      1     1     10    2013-1,\n",
       " None,\n",
       "    id        date  store  item\n",
       " 0   0  2018-01-01      1     1\n",
       " 1   1  2018-01-02      1     1\n",
       " 2   2  2018-01-03      1     1\n",
       " 3   3  2018-01-04      1     1\n",
       " 4   4  2018-01-05      1     1,\n",
       " None,\n",
       "    id  sales\n",
       " 0   0     52\n",
       " 1   1     52\n",
       " 2   2     52\n",
       " 3   3     52\n",
       " 4   4     52,\n",
       " None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded files\n",
    "folder = '/Users/kylewong/Downloads/demand-forecasting-kernels-only'\n",
    "train_path = folder + '/train.csv'\n",
    "test_path = folder + '/test.csv'\n",
    "sample_submission_path = folder + '/sample_submission.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "sample_submission_df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "train_info = train_df.info()\n",
    "test_info = test_df.info()\n",
    "sample_submission_info = sample_submission_df.info()\n",
    "\n",
    "train_df.head(), train_info, test_df.head(), test_info, sample_submission_df.head(), sample_submission_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Add a 'week' column representing the year and week number\n",
    "train_df['year_week'] = train_df['date'].dt.isocalendar().year.astype(str) + '-' + train_df['date'].dt.isocalendar().week.astype(str)\n",
    "\n",
    "# Aggregate sales by store, item, and week\n",
    "weekly_sales = train_df.groupby(['store', 'item', 'year_week'], as_index=False).agg({'sales': 'sum'})\n",
    "\n",
    "# Aggregate sales across all stores for each item and week\n",
    "aggregated_sales = weekly_sales.groupby(['item', 'year_week'], as_index=False).agg({'sales': 'sum'})\n",
    "\n",
    "# Pivot the data to create a time-series format for each item\n",
    "pivot_data = aggregated_sales.pivot(index='item', columns='year_week', values='sales').fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for RNN with 12 weeks as input\n",
    "time_steps = 12  # Using 12 weeks as input\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(scaled_data.shape[1] - time_steps - 8):  # Exclude the last 8 weeks for prediction\n",
    "    X.append(scaled_data[:, i:i + time_steps])  # Last 12 weeks as input\n",
    "    y.append(scaled_data[:, i + time_steps:i + time_steps + 8].T)  # Next 8 weeks as output\n",
    "\n",
    "X = np.array(X).reshape(-1, time_steps, 1)  # Shape: [samples, time_steps, features]\n",
    "y = np.array(y).reshape(-1, 8)  # Shape: [samples, output_steps]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 00:09:34.677238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-01 00:09:34.891299: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/302 [==============================] - ETA: 0s - loss: 0.0930 - mae: 0.2631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 00:11:41.752158: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/302 [==============================] - 132s 431ms/step - loss: 0.0930 - mae: 0.2631 - val_loss: 0.0883 - val_mae: 0.2571 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 130s 432ms/step - loss: 0.0871 - mae: 0.2564 - val_loss: 0.0884 - val_mae: 0.2587 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0867 - mae: 0.2566 - val_loss: 0.0871 - val_mae: 0.2572 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0856 - mae: 0.2539 - val_loss: 0.0862 - val_mae: 0.2536 - lr: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0852 - mae: 0.2515 - val_loss: 0.0858 - val_mae: 0.2540 - lr: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0832 - mae: 0.2491 - val_loss: 0.0842 - val_mae: 0.2494 - lr: 0.0100\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0810 - mae: 0.2438 - val_loss: 0.0820 - val_mae: 0.2438 - lr: 0.0100\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 130s 431ms/step - loss: 0.0797 - mae: 0.2414 - val_loss: 0.0792 - val_mae: 0.2404 - lr: 0.0100\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0771 - mae: 0.2364 - val_loss: 0.0760 - val_mae: 0.2343 - lr: 0.0100\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0756 - mae: 0.2331 - val_loss: 0.0749 - val_mae: 0.2321 - lr: 0.0100\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0731 - mae: 0.2284 - val_loss: 0.0738 - val_mae: 0.2290 - lr: 0.0010\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 128s 425ms/step - loss: 0.0729 - mae: 0.2274 - val_loss: 0.0737 - val_mae: 0.2285 - lr: 0.0010\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0727 - mae: 0.2269 - val_loss: 0.0735 - val_mae: 0.2283 - lr: 0.0010\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0726 - mae: 0.2267 - val_loss: 0.0735 - val_mae: 0.2278 - lr: 0.0010\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0726 - mae: 0.2263 - val_loss: 0.0735 - val_mae: 0.2280 - lr: 0.0010\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0725 - mae: 0.2260 - val_loss: 0.0734 - val_mae: 0.2268 - lr: 0.0010\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0724 - mae: 0.2257 - val_loss: 0.0734 - val_mae: 0.2277 - lr: 0.0010\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 129s 426ms/step - loss: 0.0724 - mae: 0.2256 - val_loss: 0.0732 - val_mae: 0.2268 - lr: 0.0010\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0723 - mae: 0.2254 - val_loss: 0.0731 - val_mae: 0.2263 - lr: 0.0010\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0723 - mae: 0.2253 - val_loss: 0.0730 - val_mae: 0.2263 - lr: 0.0010\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0720 - mae: 0.2248 - val_loss: 0.0729 - val_mae: 0.2260 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0720 - mae: 0.2247 - val_loss: 0.0728 - val_mae: 0.2259 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0719 - mae: 0.2246 - val_loss: 0.0728 - val_mae: 0.2259 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.2246\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0719 - mae: 0.2246 - val_loss: 0.0728 - val_mae: 0.2258 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0719 - mae: 0.2245 - val_loss: 0.0728 - val_mae: 0.2259 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0719 - mae: 0.2246 - val_loss: 0.0728 - val_mae: 0.2258 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0719 - mae: 0.2245 - val_loss: 0.0728 - val_mae: 0.2257 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0719 - mae: 0.2245 - val_loss: 0.0728 - val_mae: 0.2258 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.2244\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0719 - mae: 0.2244 - val_loss: 0.0728 - val_mae: 0.2258 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00010000000000000002.\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0719 - mae: 0.2244 - val_loss: 0.0728 - val_mae: 0.2257 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0728 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0718 - mae: 0.2244 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 46/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 47/50\n",
      "302/302 [==============================] - 127s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 48/50\n",
      "302/302 [==============================] - 128s 424ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 49/50\n",
      "302/302 [==============================] - 128s 422ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "Epoch 50/50\n",
      "302/302 [==============================] - 128s 423ms/step - loss: 0.0718 - mae: 0.2243 - val_loss: 0.0727 - val_mae: 0.2257 - lr: 1.0000e-06\n",
      "Model trained and saved as 'rnn_sales_forecast_model_12_weeks_with_lr_schedule.h5'.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# Define the learning rate scheduler to start with a higher learning rate\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 1e-2\n",
    "    decay_rate = 0.1\n",
    "    decay_epoch = 10  # After 10 epochs, reduce learning rate\n",
    "    return initial_lr * math.pow(decay_rate, math.floor(epoch / decay_epoch))\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(32, activation='relu', return_sequences=False),\n",
    "    Dense(8)  # Predict the next 8 weeks\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop, reduce_lr, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('rnn_sales_forecast_model_12_weeks_with_lr_schedule.h5')\n",
    "\n",
    "print(\"Model trained and saved as 'rnn_sales_forecast_model_12_weeks_with_lr_schedule.h5'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 40ms/step\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the train data\n",
    "train_data = pd.read_csv(train_path)\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "train_data['year_week'] = train_data['date'].dt.isocalendar().year.astype(str) + '-' + train_data['date'].dt.isocalendar().week.astype(str)\n",
    "\n",
    "# Aggregate weekly sales for each item\n",
    "weekly_sales = train_data.groupby(['item', 'year_week'], as_index=False).agg({'sales': 'sum'})\n",
    "\n",
    "# Pivot data to create a time-series format\n",
    "pivot_data = weekly_sales.pivot(index='item', columns='year_week', values='sales').fillna(0)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(pivot_data)\n",
    "\n",
    "# Extract the last 12 weeks for each item\n",
    "last_12_weeks = scaled_data[:, -12:]\n",
    "\n",
    "# Predict the next 4 weeks of sales\n",
    "predicted_scaled = model.predict(last_12_weeks.reshape(last_12_weeks.shape[0], 12, 1))\n",
    "\n",
    "# Scale back the predictions using MinMaxScaler parameters\n",
    "predicted_sales = predicted_scaled * (scaler.data_max_[-1] - scaler.data_min_[-1]) + scaler.data_min_[-1]\n",
    "\n",
    "# Extract the first 4 weeks of predictions\n",
    "predicted_sales = predicted_sales[:, :4]\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame(predicted_sales, columns=[\n",
    "    'sales_forecast_week_1', 'sales_forecast_week_2', 'sales_forecast_week_3', 'sales_forecast_week_4'\n",
    "])\n",
    "submission.insert(0, 'item', pivot_data.index)\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05627706, 0.04761905, 0.06060606, ..., 0.04761905, 0.02597403,\n",
       "        0.03896104],\n",
       "       [0.04761905, 0.06060606, 0.05627706, ..., 0.02597403, 0.03896104,\n",
       "        0.05627706],\n",
       "       [0.06060606, 0.05627706, 0.04329004, ..., 0.03896104, 0.05627706,\n",
       "        0.04761905],\n",
       "       ...,\n",
       "       [0.37662338, 0.44155844, 0.38095238, ..., 0.23809524, 0.27705628,\n",
       "        0.31601732],\n",
       "       [0.44155844, 0.38095238, 0.39393939, ..., 0.27705628, 0.31601732,\n",
       "        0.34632035],\n",
       "       [0.38095238, 0.39393939, 0.37662338, ..., 0.31601732, 0.34632035,\n",
       "        0.3030303 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
